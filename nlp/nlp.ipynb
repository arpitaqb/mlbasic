{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0e4e1c",
   "metadata": {},
   "source": [
    "### Text Preprocessing for cleaning the input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e5335",
   "metadata": {},
   "source": [
    "### Tokenization in nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033b9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c97c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello! welcome to basic learning of nlp's concept. \n",
    "we are going to learn about nlp.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2855691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', \"welcome to basic learning of nlp's concept.\", 'we are going to learn about nlp.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence tokenization : convert a paragraph into sentences\n",
    "from nltk import sent_tokenize\n",
    "sen = sent_tokenize(corpus)\n",
    "print(sen)\n",
    "type(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c2f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "welcome to basic learning of nlp's concept.\n",
      "we are going to learn about nlp.\n"
     ]
    }
   ],
   "source": [
    "for i in sen:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1043f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'welcome', 'to', 'basic', 'learning', 'of', 'nlp', \"'s\", 'concept', '.', 'we', 'are', 'going', 'to', 'learn', 'about', 'nlp', '.']\n"
     ]
    }
   ],
   "source": [
    "#WORD TOKENIZATION : convert a sentence into words\n",
    "from nltk import word_tokenize\n",
    "word = word_tokenize(corpus)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148b075b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "!\n",
      "welcome\n",
      "to\n",
      "basic\n",
      "learning\n",
      "of\n",
      "nlp\n",
      "'s\n",
      "concept\n",
      ".\n",
      "we\n",
      "are\n",
      "going\n",
      "to\n",
      "learn\n",
      "about\n",
      "nlp\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in word:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80ee4f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'welcome', 'to', 'basic', 'learning', 'of', 'nlp', \"'\", 's', 'concept', '.', 'we', 'are', 'going', 'to', 'learn', 'about', 'nlp', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import wordpunct_tokenize # ' will be treated as a separate token \n",
    "word_punct = wordpunct_tokenize(corpus)\n",
    "print(word_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d00af3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'welcome', 'to', 'basic', 'learning', 'of', 'nlp', \"'s\", 'concept.', 'we', 'are', 'going', 'to', 'learn', 'about', 'nlp', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer #. is not considered seperate word but at the end of word if there is a full stop then it will be considered as a separate token\n",
    "token = TreebankWordTokenizer()\n",
    "print(token.tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed9b21",
   "metadata": {},
   "source": [
    "## steaming  \n",
    "in steaming many words convert into some undefined words which lead to change the meaning of that word it is major isuue "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b49185",
   "metadata": {},
   "source": [
    "### portersteammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e34f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "011a556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = ['running', 'run', 'runs', 'eat', 'eats', 'eating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "136236c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running---run\n",
      "run---run\n",
      "runs---run\n",
      "eat---eat\n",
      "eats---eat\n",
      "eating---eat\n"
     ]
    }
   ],
   "source": [
    "steaming = PorterStemmer()\n",
    "for i in word:\n",
    "    print(i+ \"---\" +steaming.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "639866d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steaming.stem(\"congratulation\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92cdddf",
   "metadata": {},
   "source": [
    "### regexstemmer class \n",
    "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a43dd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3676fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stem = RegexpStemmer('ing$|s$|es$|ed$', min=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "342417f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runn'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5302cc",
   "metadata": {},
   "source": [
    "### snowball stemmer : more accurate than porter stemmer\n",
    "It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4968860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0360962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5e7293d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstem.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66fdd220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steaming.stem(\"fairly\"),steaming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cc4efed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstem.stem(\"fairly\"),snowballstem.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae7e5a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('histori', 'goe')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstem.stem(\"history\"),snowballstem.stem(\"goes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8c5627e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('histori', 'goe')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steaming.stem(\"history\"),steaming.stem(\"goes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517d300",
   "metadata": {},
   "source": [
    "### lemmatization \n",
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’ which is a root word rather than root stem. After lemmatization, we will be getting a valid word which meaning will not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91d19037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61c6bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f56dbc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\User1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'based on pos tag the output will be defined\\npos:\\nnoun - n\\nverb -v\\nadjective - a\\nadverb -r'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') \n",
    "\"\"\"based on pos tag the output will be defined\n",
    "pos:\n",
    "noun - n\n",
    "verb -v\n",
    "adjective - a\n",
    "adverb -r\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20f695f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "good\n",
      "well\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"running\", pos='n'))\n",
    "print(lemmatizer.lemmatize(\"running\", pos='v'))\n",
    "print(lemmatizer.lemmatize(\"better\", pos='a'))\n",
    "print(lemmatizer.lemmatize(\"better\", pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d29d14da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\", pos='v'),lemmatizer.lemmatize(\"sportingly\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a252e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('history', 'go')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"history\"),lemmatizer.lemmatize(\"goes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d264fe",
   "metadata": {},
   "source": [
    "### stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4773e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello! welcome to basic learning of nlp's concept. \n",
    "we are going to learn about nlp.\n",
    "NLP is widely used in chatbots and virtual assistants.\n",
    "It helps computers understand human language.\n",
    "Tokenization, stemming, and lemmatization are important steps in NLP.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6f83d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5c18b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "227d6da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da9b18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c112eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!',\n",
       " \"welcome to basic learning of nlp's concept.\",\n",
       " 'we are going to learn about nlp.',\n",
       " 'NLP is widely used in chatbots and virtual assistants.',\n",
       " 'It helps computers understand human language.',\n",
       " 'Tokenization, stemming, and lemmatization are important steps in NLP.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = nltk.sent_tokenize(corpus)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea14a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply stopwords to corpus and then filter and after that apply steaming\n",
    "\n",
    "for i in range(len(sen)): #iterate through sentences\n",
    "    word = nltk.word_tokenize(sen[i]) # tokenize each sentence into words\n",
    "    words = [stemmer.stem(w) for w in word if w not in stopwords.words('english')] # filter out stopwords and apply stemming\n",
    "    sen[i] = ' '.join(words) # join the words back into a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "620c5b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello !',\n",
       " \"welcom basic learn nlp 's concept .\",\n",
       " 'go learn nlp .',\n",
       " 'nlp wide use chatbot virtual assist .',\n",
       " 'it help comput understand human languag .',\n",
       " 'token , stem , lemmat import step nlp .']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2abc0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstm = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3201d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sen)):\n",
    "    word = nltk.word_tokenize(sen[i]) \n",
    "    words = [snowballstm.stem(w) for w in word if w not in stopwords.words('english')]\n",
    "    sen[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9516ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello !',\n",
       " \"welcom basic learn nlp 's concept .\",\n",
       " 'go learn nlp .',\n",
       " 'nlp wide use chatbot virtual assist .',\n",
       " 'help comput understand human languag .',\n",
       " 'token , stem , lemmat import step nlp .']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "71414b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lamatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbc2ddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello !',\n",
       " \"welcom basic learn nlp 's concept .\",\n",
       " 'go learn nlp .',\n",
       " 'nlp wide use chatbot virtual assist .',\n",
       " 'help comput understand human languag .',\n",
       " 'token , stem , lemmat import step nlp .']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(sen)):\n",
    "    word = nltk.word_tokenize(sen[i])\n",
    "    words = [lamatizer.lemmatize(w, pos='v') for w in word if w not in stopwords.words('english')]\n",
    "    sen[i] = ' '.join(words)\n",
    "\n",
    "sen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1d109",
   "metadata": {},
   "source": [
    "### POS tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "14f171e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e75012a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!',\n",
       " \"welcome to basic learning of nlp's concept.\",\n",
       " 'we are going to learn about nlp.',\n",
       " 'NLP is widely used in chatbots and virtual assistants.',\n",
       " 'It helps computers understand human language.',\n",
       " 'Tokenization, stemming, and lemmatization are important steps in NLP.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = nltk.sent_tokenize(corpus)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "603be7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9fc97c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NN'), ('!', '.')]\n",
      "[('welcome', 'JJ'), ('basic', 'JJ'), ('learning', 'VBG'), ('nlp', 'NN'), (\"'s\", 'POS'), ('concept', 'NN'), ('.', '.')]\n",
      "[('going', 'VBG'), ('learn', 'JJ'), ('nlp', 'NN'), ('.', '.')]\n",
      "[('NLP', 'NNP'), ('widely', 'RB'), ('used', 'VBD'), ('chatbots', 'NNS'), ('virtual', 'JJ'), ('assistants', 'NNS'), ('.', '.')]\n",
      "[('It', 'PRP'), ('helps', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n",
      "[('Tokenization', 'NN'), (',', ','), ('stemming', 'VBG'), (',', ','), ('lemmatization', 'NN'), ('important', 'JJ'), ('steps', 'NNS'), ('NLP', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sen)):\n",
    "    word = nltk.word_tokenize(sen[i])\n",
    "    words = [w for w in word if w not in stopwords.words('english')]\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04c26298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('test', 'NN'),\n",
       " ('sentence', 'NN')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(\"this is a test sentence\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be70d2",
   "metadata": {},
   "source": [
    "### tree for pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8735124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize('this is a test sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afe974b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9205e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\User1/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "416e1f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\User1/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eaa7a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0230a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
