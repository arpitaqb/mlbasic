{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ce27cc",
   "metadata": {},
   "source": [
    "### Text preprocessing input to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0891445",
   "metadata": {},
   "source": [
    "### bag of words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1741996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da009110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Huh y lei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham                                       Huh y lei...\n",
       "1  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "2  spam  This is the 2nd time we have tried 2 contact u...\n",
       "3   ham               Will ü b going to esplanade fr home?\n",
       "4   ham  Pity, * was in mood for that. So...any other s...\n",
       "5  spam  WINNER!! As a valued network customer you have...\n",
       "6   ham  The guy did some bitching but I acted like i'd...\n",
       "7   ham                         Rofl. Its true to its name\n",
       "8  spam  URGENT! You have won a 1 week FREE membership ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from a text file\n",
    "# The file is expected to be tab-separated with two columns: 'label' and 'message'\n",
    "# The first column is the label (spam or ham) and the second column is the message text\n",
    "txt = pd.read_csv('SMSSpamCollection.txt',sep='\\t', names=['label', 'message'])\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca06816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b721ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "porterstem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(len(txt)):\n",
    "    msg = re.sub('[^a-zA-Z]', ' ', txt['message'][i]) # Remove non-alphabetic characters\n",
    "    msg = msg.lower() # Convert to lowercase\n",
    "    msg = msg.split() # Split into words\n",
    "    msg = [porterstem.stem(w) for w in msg if w not in set(stopwords.words('english'))] # Remove stopwords and apply stemming\n",
    "    msg = ' '.join(msg) # Join the words back into a single string\n",
    "    corpus.append(msg) # append to the corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e511f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['huh lei',\n",
       " 'remind get pound free call credit detail great offer pl repli text valid name hous postcod',\n",
       " 'nd time tri contact u u pound prize claim easi call p per minut bt nation rate',\n",
       " 'b go esplanad fr home',\n",
       " 'piti mood suggest',\n",
       " 'winner valu network custom select receivea prize reward claim call claim code kl valid hour',\n",
       " 'guy bitch act like interest buy someth els next week gave us free',\n",
       " 'rofl true name',\n",
       " 'urgent week free membership prize jackpot txt word claim c www dbuk net lccltd pobox ldnw rw']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c8d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.feature_extraction.text import CountVectorizer # count vectorizer convert text to bag of words\n",
    "# We will use a binary count vectorizer that only counts the presence or absence of words\n",
    "# and limits the maximum number of features to 100 for 100 most frequent words\n",
    "cv = CountVectorizer(max_features=100, binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cv.fit_transform(corpus).toarray()\n",
    "# Convert the labels to a binary format (0 for ham, 1 for spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df6c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=10, linewidth=1000, formatter=dict(float= lambda x: \"%.3g\" % x)) # Set print options for better readability\n",
    "x # Convert labels to binary format numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ebac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'huh': np.int64(25),\n",
       " 'lei': np.int64(31),\n",
       " 'remind': np.int64(52),\n",
       " 'get': np.int64(18),\n",
       " 'pound': np.int64(48),\n",
       " 'free': np.int64(16),\n",
       " 'call': np.int64(4),\n",
       " 'credit': np.int64(8),\n",
       " 'detail': np.int64(11),\n",
       " 'great': np.int64(20),\n",
       " 'offer': np.int64(42),\n",
       " 'pl': np.int64(45),\n",
       " 'repli': np.int64(53),\n",
       " 'text': np.int64(60),\n",
       " 'valid': np.int64(67),\n",
       " 'name': np.int64(36),\n",
       " 'hous': np.int64(24),\n",
       " 'postcod': np.int64(47),\n",
       " 'nd': np.int64(38),\n",
       " 'time': np.int64(61),\n",
       " 'tri': np.int64(62),\n",
       " 'contact': np.int64(7),\n",
       " 'prize': np.int64(49),\n",
       " 'claim': np.int64(5),\n",
       " 'easi': np.int64(12),\n",
       " 'per': np.int64(43),\n",
       " 'minut': np.int64(34),\n",
       " 'bt': np.int64(2),\n",
       " 'nation': np.int64(37),\n",
       " 'rate': np.int64(50),\n",
       " 'go': np.int64(19),\n",
       " 'esplanad': np.int64(14),\n",
       " 'fr': np.int64(15),\n",
       " 'home': np.int64(22),\n",
       " 'piti': np.int64(44),\n",
       " 'mood': np.int64(35),\n",
       " 'suggest': np.int64(59),\n",
       " 'winner': np.int64(70),\n",
       " 'valu': np.int64(68),\n",
       " 'network': np.int64(40),\n",
       " 'custom': np.int64(9),\n",
       " 'select': np.int64(57),\n",
       " 'receivea': np.int64(51),\n",
       " 'reward': np.int64(54),\n",
       " 'code': np.int64(6),\n",
       " 'kl': np.int64(28),\n",
       " 'hour': np.int64(23),\n",
       " 'guy': np.int64(21),\n",
       " 'bitch': np.int64(1),\n",
       " 'act': np.int64(0),\n",
       " 'like': np.int64(32),\n",
       " 'interest': np.int64(26),\n",
       " 'buy': np.int64(3),\n",
       " 'someth': np.int64(58),\n",
       " 'els': np.int64(13),\n",
       " 'next': np.int64(41),\n",
       " 'week': np.int64(69),\n",
       " 'gave': np.int64(17),\n",
       " 'us': np.int64(66),\n",
       " 'rofl': np.int64(55),\n",
       " 'true': np.int64(63),\n",
       " 'urgent': np.int64(65),\n",
       " 'membership': np.int64(33),\n",
       " 'jackpot': np.int64(27),\n",
       " 'txt': np.int64(64),\n",
       " 'word': np.int64(71),\n",
       " 'www': np.int64(72),\n",
       " 'dbuk': np.int64(10),\n",
       " 'net': np.int64(39),\n",
       " 'lccltd': np.int64(29),\n",
       " 'pobox': np.int64(46),\n",
       " 'ldnw': np.int64(30),\n",
       " 'rw': np.int64(56)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_  #showes dictionary of all n-grams which are used as features in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bag of Words with ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=100, binary=True, ngram_range=(2, 3)) # Create a CountVectorizer with n-grams (bigrams and trigrams)\n",
    "# ngram_range=(2, 3) means we will consider both bigrams and trigrams\n",
    "x = cv.fit_transform(corpus).toarray() # Convert the corpus to a bag of words with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f6a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'huh lei': np.int64(56),\n",
       " 'get pound': np.int64(47),\n",
       " 'pound free': np.int64(94),\n",
       " 'free call': np.int64(41),\n",
       " 'call credit': np.int64(10),\n",
       " 'credit detail': np.int64(26),\n",
       " 'detail great': np.int64(32),\n",
       " 'great offer': np.int64(51),\n",
       " 'offer pl': np.int64(84),\n",
       " 'pl repli': np.int64(90),\n",
       " 'name hous': np.int64(73),\n",
       " 'hous postcod': np.int64(55),\n",
       " 'get pound free': np.int64(48),\n",
       " 'pound free call': np.int64(95),\n",
       " 'free call credit': np.int64(42),\n",
       " 'call credit detail': np.int64(11),\n",
       " 'credit detail great': np.int64(27),\n",
       " 'detail great offer': np.int64(33),\n",
       " 'great offer pl': np.int64(52),\n",
       " 'offer pl repli': np.int64(85),\n",
       " 'pl repli text': np.int64(91),\n",
       " 'name hous postcod': np.int64(74),\n",
       " 'nd time': np.int64(76),\n",
       " 'contact pound': np.int64(24),\n",
       " 'pound prize': np.int64(96),\n",
       " 'prize claim': np.int64(98),\n",
       " 'claim easi': np.int64(18),\n",
       " 'easi call': np.int64(34),\n",
       " 'call per': np.int64(12),\n",
       " 'per minut': np.int64(86),\n",
       " 'minut bt': np.int64(70),\n",
       " 'bt nation': np.int64(4),\n",
       " 'nation rate': np.int64(75),\n",
       " 'nd time tri': np.int64(77),\n",
       " 'contact pound prize': np.int64(25),\n",
       " 'pound prize claim': np.int64(97),\n",
       " 'prize claim easi': np.int64(99),\n",
       " 'claim easi call': np.int64(19),\n",
       " 'easi call per': np.int64(35),\n",
       " 'call per minut': np.int64(13),\n",
       " 'per minut bt': np.int64(87),\n",
       " 'minut bt nation': np.int64(71),\n",
       " 'bt nation rate': np.int64(5),\n",
       " 'go esplanad': np.int64(49),\n",
       " 'esplanad fr': np.int64(38),\n",
       " 'fr home': np.int64(40),\n",
       " 'go esplanad fr': np.int64(50),\n",
       " 'esplanad fr home': np.int64(39),\n",
       " 'piti mood': np.int64(88),\n",
       " 'mood suggest': np.int64(72),\n",
       " 'piti mood suggest': np.int64(89),\n",
       " 'network custom': np.int64(80),\n",
       " 'custom select': np.int64(28),\n",
       " 'claim call': np.int64(14),\n",
       " 'call claim': np.int64(8),\n",
       " 'claim code': np.int64(16),\n",
       " 'code kl': np.int64(22),\n",
       " 'kl valid': np.int64(61),\n",
       " 'network custom select': np.int64(81),\n",
       " 'custom select receivea': np.int64(29),\n",
       " 'claim call claim': np.int64(15),\n",
       " 'call claim code': np.int64(9),\n",
       " 'claim code kl': np.int64(17),\n",
       " 'code kl valid': np.int64(23),\n",
       " 'kl valid hour': np.int64(62),\n",
       " 'guy bitch': np.int64(53),\n",
       " 'bitch act': np.int64(2),\n",
       " 'act like': np.int64(0),\n",
       " 'like interest': np.int64(66),\n",
       " 'interest buy': np.int64(57),\n",
       " 'buy someth': np.int64(6),\n",
       " 'els next': np.int64(36),\n",
       " 'next week': np.int64(82),\n",
       " 'gave us': np.int64(45),\n",
       " 'guy bitch act': np.int64(54),\n",
       " 'bitch act like': np.int64(3),\n",
       " 'act like interest': np.int64(1),\n",
       " 'like interest buy': np.int64(67),\n",
       " 'interest buy someth': np.int64(58),\n",
       " 'buy someth els': np.int64(7),\n",
       " 'els next week': np.int64(37),\n",
       " 'next week gave': np.int64(83),\n",
       " 'gave us free': np.int64(46),\n",
       " 'free membership': np.int64(43),\n",
       " 'membership prize': np.int64(68),\n",
       " 'jackpot txt': np.int64(59),\n",
       " 'claim www': np.int64(20),\n",
       " 'dbuk net': np.int64(30),\n",
       " 'net lccltd': np.int64(78),\n",
       " 'lccltd pobox': np.int64(63),\n",
       " 'pobox ldnw': np.int64(92),\n",
       " 'ldnw rw': np.int64(65),\n",
       " 'free membership prize': np.int64(44),\n",
       " 'membership prize jackpot': np.int64(69),\n",
       " 'jackpot txt word': np.int64(60),\n",
       " 'claim www dbuk': np.int64(21),\n",
       " 'dbuk net lccltd': np.int64(31),\n",
       " 'net lccltd pobox': np.int64(79),\n",
       " 'lccltd pobox ldnw': np.int64(64),\n",
       " 'pobox ldnw rw': np.int64(93)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_ # Display the vocabulary created by the CountVectorizer it will show biagram and trigram features cz we set ngram_range=(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f54b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc5a38",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd51c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fe83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ['i like nlp', 'nlp is fun', 'i like python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(doc) # it will give 2d matrix document x vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c406ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fun', 'is', 'like', 'nlp', 'python'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e736d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.70710678, 0.70710678, 0.        ],\n",
       "       [0.62276601, 0.62276601, 0.        , 0.4736296 , 0.        ],\n",
       "       [0.        , 0.        , 0.60534851, 0.        , 0.79596054]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.toarray() # Display the TF-IDF matrix as a NumPy array \n",
    "#it will show array of (3, 5) shape where 3 is number of documents and 5 is number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39471d",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc2cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce3181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2,2)) # Create a TfidfVectorizer with bigrams (minimum 2 words, maximum 2 words)\n",
    "x = vectorizer.fit_transform(doc) # Convert the documents to a TF-IDF matrix with bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462665a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['is fun', 'like nlp', 'like python', 'nlp is'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out() # Get the feature names (words) in the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , 0.        ],\n",
       "       [0.70710678, 0.        , 0.        , 0.70710678],\n",
       "       [0.        , 0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray() # Display the TF-IDF matrix as a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871fde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
